#!/usr/bin/python

import argparse, binascii, collections, errno, hashlib, fcntl, os, os.path
import select, stat, string, sqlite3, subprocess, sys, tempfile, time

import tbl.ddar_pb2, tbl.dds
import tbl.netstring as netstring

def _set_nonblocking(fileobj):
    fd = fileobj.fileno()
    fl = fcntl.fcntl(fd, fcntl.F_GETFL)
    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)

def _read_some(fileobj):
    while True:
        try:
            # Attempt an initial read before a select. It seems that Python
            # can hold some data buffered from a previous read, causing a
            # select to block indefinitely even though there is data available
            # because the fileobj has read it in already. At least this is
            # my best guess based on the behaviour I was seeing.
            data = fileobj.read()
        except IOError, e:
            if e.errno in (errno.EAGAIN, errno.EWOULDBLOCK):
                select.select([fileobj.fileno()], [], [], None)
            elif e.errno == errno.EINTR:
                pass
            else:
                raise
        else:   
            return data

class _ImmediateRequest(object):
    def __init__(self, reply):
        self.reply = reply

    def flush(self): pass

class _WorkPipeline(object):
    def __init__(self, size, in_fn, out_fn):
        self.size = size
        self.in_fn = in_fn
        self.out_fn = out_fn

        self.q = collections.deque()

    def _push(self, item):
        self.q.append(self.in_fn(item))
    
    def _pop(self):
        self.out_fn(*self.q.popleft())

    def _flush(self):
        while self.q:
            self._pop()

    def feed_and_flush(self, src):
        '''Given an iterable src, this will call work = in_fn(item) and then
        out_fn(*work) for item in src, interleaving the calls such that size
        work is in progress at a time.'''

        src = iter(src)

        # Fill the pipeline
        for i in xrange(self.size):
            try:
                item = src.next()
            except StopIteration:
                # Source emptied before pipeline filled; just flush the
                # remaining items back out
                self._flush()
                return
            self._push(item)

        # Work the pipeline
        for item in src:
            self._push(item)
            self._pop()

        # Source is now empty: flush the final items
        self._flush()

class Archive(object):
    def __init__(self, dirname, auto_create=False):
        self.dirname = dirname

        if not os.path.exists(self.dirname):
            if auto_create:
                self._create()
            else:
                raise RuntimeError("archive %s not found" % self.dirname)
        elif not os.path.isdir(self.dirname):
            raise RuntimeError("%s exists but is not a ddar directory" %
                               self.dirname)

        try:
            format_name = self._read_small_file(self._format_filename('name'))
        except:
            raise RuntimeError('%s is not a ddar directory' %
                               self.dirname)
        else:
            if format_name.lstrip().rstrip() != 'ddar':
                raise RuntimeError('%s is not a ddar directory' %
                                   self.dirname)

        version = self._read_small_file(self._format_filename('version'))
        version = version.lstrip().rstrip()
        if version != '1':
            raise RuntimeError('%s uses ddar archive version %s but only' +
                               'version 1 is supported' % self.dirname)

        self.db = sqlite3.connect(os.path.join(self.dirname, 'db'))
        self.db.text_factory = str
        self.db.execute('PRAGMA foreign_keys = ON')

    @staticmethod
    def _read_small_file(name, size_limit=1024):
        with open(name, 'r') as f:
            data = f.read(size_limit)
        return data

    @staticmethod
    def _write_small_file(name, data):
        with open(name, 'w') as f:
            f.write(data)

    def _create(self):
        os.mkdir(self.dirname)
        os.mkdir(os.path.join(self.dirname, 'format'))
        os.mkdir(os.path.join(self.dirname, 'objects'))

        self._write_small_file(self._format_filename('name'), "ddar\n")
        self._write_small_file(self._format_filename('version'), "1\n")

        db = sqlite3.connect(os.path.join(self.dirname, 'db'))
        c = db.cursor()
        c.execute('''
CREATE TABLE member (id INTEGER PRIMARY KEY,
                      name TEXT UNIQUE NOT NULL,
                      length INTEGER,
                      hash BLOB,
                      create_time INTEGER)''')
        c.execute('''
CREATE TABLE chunk (member_id INTEGER NOT NULL,
                    hash BLOB NOT NULL,
                    offset INTEGER NOT NULL,
                    length INTEGER NOT NULL,
                    UNIQUE (member_id, offset),
                    FOREIGN KEY (member_id) REFERENCES member(id))''')
        # No need for a (tag,offset) index as we have a UNIQUE
        # constraint on it. This causes sqlite to generate an index and
        # the documentatation states that there is a performance degradation
        # by having an extra index instead of relying on this fact.
        c.execute('CREATE INDEX chunk_hash_idx ON chunk(hash)')
        c.execute('CREATE INDEX member_create_time_idx ON member(create_time)')
        c.close()
        db.commit()
        db.close()

    def _object_filename(self, h):
        h = binascii.hexlify(h)
        return os.path.join(self.dirname, 'objects', h[0:2], h[2:])

    def _makedirs(self, object_filename):
        try:
            os.makedirs(os.path.dirname(object_filename))
        except OSError, e:
            if e.errno != errno.EEXIST:
                raise

    def _removedirs(self, object_filename):
        try:
            os.removedirs(os.path.dirname(object_filename))
        except OSError, e:
            if e.errno != errno.ENOTEMPTY:
                raise

    def _format_filename(self, n):
        return os.path.join(self.dirname, 'format', n)

    def close(self): pass

    @staticmethod
    def _have_chunk(cursor, h):
        h_blob = buffer(h) # buffer to make sqlite use a BLOB
        cursor.execute('SELECT 1 FROM chunk WHERE hash=? LIMIT 1', (h_blob,))
        return _ImmediateRequest(bool(cursor.fetchone()))

    def _store_chunk(self, member_id, cursor, data, offset, length,
                     sha256=None):
        '''Store the chunk in the database and the object store if necessary,
        but do not commit. The database insert is done last, so that if
        interrupted the database is never wrong. At worst a dangling object
        will be left in the object store.
        
        If sha256 is provided, then data can be None, in which case the chunk
        must be in the object store already. If sha256 and data are both
        provided, then they must match.'''

        if data is None:
            assert(sha256 is not None)
            h = sha256
        else:
            h = hashlib.sha256(data).digest()
            assert(sha256 is None or sha256 == h)
            assert(len(data) == length)

        if not self._have_chunk(cursor, h).reply:
            assert(data is not None)
            object_filename = self._object_filename(h)
            object_dir = os.path.dirname(object_filename)
            self._makedirs(object_filename)
            temp = tempfile.NamedTemporaryFile(dir=object_dir, delete=False)
            try:
                temp.write(data)
            finally:
                temp.close()
            os.rename(temp.name, object_filename)

        h_blob = buffer(h)
        cursor.execute('INSERT INTO chunk ' +
                       '(member_id, hash, offset, length) ' +
                       'VALUES (?, ?, ?, ?)',
                       (member_id, h_blob, offset, length))

    @staticmethod
    def _store_add_member(cursor, tag):
        cursor.execute('SELECT 1 FROM member WHERE name=? LIMIT 1', (tag,))
        if cursor.fetchone():
            raise RuntimeError("member %s already exists" % tag)

        cursor.execute('INSERT INTO member (name, create_time) VALUES (?, ?)',
                       (tag, int(time.time())))
        member_id = cursor.lastrowid
        cursor.execute('DELETE FROM chunk WHERE member_id=?', (member_id,))
        return member_id

    def _store_complete_member(self, cursor, h, length, member_id):
        h_blob = buffer(h)
        cursor.execute('UPDATE member SET hash=?, length=? WHERE ' +
                       'id=?', (h_blob, length, member_id))
        return _ImmediateRequest(None)

    def _store_commit(self, cursor):
        try:
            cursor.close()
        except:
            pass
        self.db.commit()

    def store(self, tag, f=sys.stdin, aio=False, server=False):
        cursor = self.db.cursor()
        member_id = self._store_add_member(cursor, tag)
        if server:
            return self._StoreRPCServer(archive=self,
                                        member_id=member_id,
                                        cursor=cursor).loop()
        else:
            try:
                self._store(cursor, member_id, f, aio)
            finally:
                self._store_commit(cursor)

    def _analyze_and_store(self, cursor, dds, member_id, pipeline_size=0):
        total_length = [0]
        full_h = hashlib.sha256()

        def in_fn(data):
            offset = total_length[0]
            length = len(data)
            chunk_h = hashlib.sha256(data).digest()

            request = self._have_chunk(cursor=cursor,
                                       h=hashlib.sha256(data).digest())

            # Update running stats
            total_length[0] += length
            full_h.update(data)
            
            return request, data, chunk_h, offset, length

        def out_fn(request, data, chunk_h, offset, length):
            if request.reply:
                data = None
            self._store_chunk(member_id=member_id,
                              cursor=cursor,
                              data=data,
                              offset=offset,
                              length=length,
                              sha256=chunk_h)

        work_pipeline = _WorkPipeline(pipeline_size, in_fn, out_fn)
        work_pipeline.feed_and_flush(dds.chunks())
            
        return total_length[0], full_h.digest()

    def _store(self, cursor, member_id, f, aio):
        dds = tbl.dds.DDS()
        dds.set_file(f)
        if aio:
            dds.set_aio()
        dds.begin()

        length, h = self._analyze_and_store(cursor, dds, member_id)
        
        self._store_complete_member(cursor=cursor,
                                    h=h,
                                    length=length,
                                    member_id=member_id).flush()

    class _StoreRPCServer(object):
        def __init__(self, archive, member_id, cursor):
            self.archive = archive
            self.member_id = member_id
            self.cursor = cursor

        def _rpc_have_chunk_request(self, req):
            reply = tbl.ddar_pb2.HaveChunkReply()
            reply.have = self.archive._have_chunk(self.cursor, req.sha256).reply
            reply.sha256 = req.sha256
            return reply

        def _rpc_store_chunk_request(self, req):
            if req.HasField('data'):
                data = req.data
            else:
                data = None

            self.archive._store_chunk(member_id=self.member_id,
                                      cursor=self.cursor,
                                      data=data,
                                      offset=req.offset,
                                      length=req.length,
                                      sha256=req.sha256)
            reply = tbl.ddar_pb2.StoreChunkReply()
            reply.sha256 = req.sha256
            return reply

        def _rpc_commit_request(self, req):
            self.archive._store_complete_member(cursor=self.cursor,
                                                h=req.sha256,
                                                length=req.length,
                                                member_id=self.member_id)
            # Commit now so that it is confirmed before the reply. Another
            # attempt will happen after EOF as well.
            self.archive._store_commit(self.cursor)
            reply = tbl.ddar_pb2.CommitReply()
            reply.sha256 = req.sha256
            return reply

        def _rpc_request(self, name, req):
            reply = getattr(self, '_rpc_' + name, req)(req)
            wrapped_reply = tbl.ddar_pb2.Reply()
            for field in wrapped_reply.DESCRIPTOR.fields:
                if field.message_type == reply.DESCRIPTOR:
                    getattr(wrapped_reply, field.name).MergeFrom(reply)
                    getattr(wrapped_reply, field.name).SetInParent()
                    break
            else:
                raise RuntimeError("Couldn't find reply field in wrapper")
            sys.stdout.write(netstring.encode(wrapped_reply.SerializeToString()))
            sys.stdout.flush()

        def loop(self):
            decoder = netstring.Decoder()
            _set_nonblocking(sys.stdin)

            try:
                data = _read_some(sys.stdin)
                while data:
                    for encoded_request in decoder.feed(data):
                        request_container = tbl.ddar_pb2.Request()
                        request_container.ParseFromString(encoded_request)
                        for req_type, req in request_container.ListFields():
                            self._rpc_request(req_type.name, req)

                    data = _read_some(sys.stdin)
            finally:
                # Same as local: commit anyway to get any partial writes. This
                # will be consistent since we are careful about storing chunk
                # object files before adding them to the DB
                
                # This could have also happened on CommitRequest but a
                # duplicate is fine
                self.archive._store_commit(self.cursor)

    def load(self, tag, f=sys.stdout):
        cursor = self.db.cursor()
        cursor.execute('SELECT id, hash FROM member WHERE name=?', (tag,))
        row = cursor.fetchone()
        if not row:
            raise RuntimeError('member %s not found in archive' % tag)

        member_id, h = row
        expected_h = str(h)
        cursor.execute('SELECT hash, offset, length ' +
                       'FROM chunk WHERE member_id=? ' +
                       'ORDER BY offset', (member_id,))

        h2 = hashlib.sha256()
        next_offset = 0
        row = cursor.fetchone()
        while row:
            h, offset, length = row
            assert(offset == next_offset)
            with open(self._object_filename(h), 'rb') as g:
                data = g.read()
            assert(len(data) == length)
            h2.update(data)
            f.write(data)

            next_offset = offset + length
            row = cursor.fetchone()

        if expected_h != h2.digest():
            raise RuntimeError('Extracted member failed hash check')

    def delete(self, tag):
        cursor = self.db.cursor()
        cursor2 = self.db.cursor()
        cursor.execute('SELECT id FROM member WHERE name=?', (tag,))
        row = cursor.fetchone()
        if not row:
            raise RuntimeError('member %s not found in archive' % tag)
        member_id = row[0]
        cursor.execute('SELECT hash FROM chunk WHERE member_id=?',
                       (member_id,))
        row = cursor.fetchone()
        while row:
            h = row[0]
            cursor2.execute('SELECT 1 FROM chunk WHERE hash=? AND ' +
                            'member_id != ? LIMIT 1', (h, member_id))
            if not cursor2.fetchone():
                try:
                    object_filename = self._object_filename(h)
                    os.unlink(object_filename)
                    self._removedirs(object_filename)
                except OSError, e:
                    # ignore ENOENT to make delete idempotent on a SIGINT
                    if e.errno != errno.ENOENT:
                        raise
            row = cursor.fetchone()
        cursor.execute('DELETE FROM chunk WHERE member_id=?', (member_id,))
        cursor.execute('DELETE FROM member WHERE id=?', (member_id,))
        cursor.close()
        self.db.commit()

    def list_tags(self):
        cursor = self.db.cursor()
        cursor.execute('SELECT name FROM member')
        tag = cursor.fetchone()
        while tag:
            yield tag[0]
            tag = cursor.fetchone()

    def get_last_tag(self):
        cursor = self.db.cursor()
        cursor.execute('SELECT name FROM member ORDER BY rowid DESC LIMIT 1')
        row = cursor.fetchone()
        return row[0] if row else None

    def suggest_tag(self):
        cursor = self.db.cursor()
        base = time.strftime('%Y-%m-%d')
        cursor.execute('SELECT 1 FROM member WHERE name=?', (base,))
        if not cursor.fetchone():
            return base
        root_like = base + '-%'
        cursor.execute('SELECT COUNT(*) FROM member WHERE name LIKE ?',
                       (root_like,))
        row = cursor.fetchone()
        if not row or not row[0]:
            return '%s-2' % base

        suffix = row[0] + 1

        cursor.execute('SELECT 1 FROM member WHERE name=?',
                       ('%s-%s' % (base, suffix),))
        row = cursor.fetchone()
        while row:
            suffix += 1
            cursor.execute('SELECT 1 FROM member WHERE name=?',
                           ('%s-%s' % (base, suffix),))
            row = cursor.fetchone()

        return '%s-%s' % (base, suffix)

    hexdigits = set(string.hexdigits) - set(string.uppercase)
    sha256_length = len(hashlib.sha256().hexdigest())

    @classmethod
    def _valid_hex_hash(cls, h):
        return (len(h) == cls.sha256_length and
            all((d in cls.hexdigits for d in h)))

    def _fsck_fs(self):
        status = True

        cursor = self.db.cursor()
        for h in os.listdir(os.path.join(self.dirname, 'objects')):
            if not self._valid_hex_hash(h):
                continue
            cursor.execute('SELECT 1 FROM chunk WHERE hash=? LIMIT 1',
                           (buffer(binascii.unhexlify(h)),))
            if not cursor.fetchone():
                print 'Unknown object %s' % h
                status = False

        return status

    def _fsck_db_to_fs_chunk(self):
        status = True

        cursor = self.db.cursor()

        # Check each hash is correct
        cursor.execute('SELECT DISTINCT hash, length FROM chunk')

        row = cursor.fetchone()
        while row:
            h, length = row
            h = str(h) # sqlite3 returns a buffer for a BLOB; we want an str;
                       # otherwise comparisons never match
            try:
                with open(self._object_filename(h), 'rb') as f:
                    data = f.read(length+1)
            except IOError:
                print "Could not read chunk %s" % binascii.hexlify(h)
                status = False
            else:
                if len(data) != length:
                    print "Chunk %s wrong size" % binascii.hexlify(h)
                    status = False
                elif hashlib.sha256(data).digest() != h:
                    print "Chunk %s corrupt" % binascii.hexlify(h)
                    status = False
            
            row = cursor.fetchone()

        return status

    def _fsck_db_to_fs_member(self):
        status = True

        cursor = self.db.cursor()
        cursor2 = self.db.cursor()
        # Check each member hash is correct
        cursor.execute('SELECT id, name, hash FROM member')
        row = cursor.fetchone()
        while row:
            member_id, tag, h = row
            h = str(h)

            cursor2.execute('SELECT hash FROM chunk WHERE member_id=? ' +
                            'ORDER BY offset', (member_id,))

            h2 = hashlib.sha256()
            row2 = cursor2.fetchone()
            while row2:
                chunk_hash = row2[0]
                try:
                    with open(self._object_filename(chunk_hash), 'rb') as f:
                        data = f.read()
                except IOError:
                    print ("Could not read chunk %s from %s" %
                            (binascii.hexlify(chunk_hash), tag))
                    status = False
                else:
                    h2.update(data)

                row2 = cursor2.fetchone()

            if h != h2.digest():
                print '%s: hash mismatch' % tag
                status = False

            row = cursor.fetchone()
        
        return status

    def _fsck_db(self):
        status = True

        cursor = self.db.cursor()
        cursor2 = self.db.cursor()

        # Check through each member in the chunk table
        cursor.execute('SELECT member_id, offset, length FROM chunk ' +
                       'ORDER BY member_id, offset')
        current_member_id = None
        row = cursor.fetchone()
        while row:
            member_id, offset, length = row
            if member_id != current_member_id:
                current_member_id = member_id
                current_offset = 0
                tag = cursor2.execute('SELECT name FROM member WHERE id=?',
                                      (member_id,)).fetchone()[0]
            if offset != current_offset:
                if offset < current_offset:
                    print ("Chunk at offset %d for %s has an overlap" %
                        (offset, tag))
                    status = False
                else:
                    assert(offset > current_offset)
                    print ("Hole in %s found between %d and %d" %
                        (tag, current_offset, offset))
                    status = False
                current_offset = offset
            current_offset += length

            row = cursor.fetchone()

        # Check all lengths in member match chunk totals
        cursor.execute('''
SELECT member.name

FROM
    member,

    (SELECT member_id, sum(length) AS length
     FROM chunk GROUP BY member_id) AS chunk_lengths

WHERE
      member.id = chunk_lengths.member_id
      AND member.length != chunk_lengths.length''')
        
        row = cursor.fetchone()
        while row:
            print 'Length mismatch in %s' % row[0]
            status = False
            row = cursor.fetchone()

        return status

    def fsck(self):
        status = True
        status = status and self._fsck_db()
        status = status and self._fsck_fs()
        status = status and self._fsck_db_to_fs_chunk()
        status = status and self._fsck_db_to_fs_member()
        return status

class RemoteArchive(Archive):
    def __init__(self, rsh, server, args):
        # Override parent completely
        remote_args = [ rsh, server, "ddar", "--server" ]
        remote_args.extend(args)
        p = subprocess.Popen(' '.join(remote_args),
                stdin=subprocess.PIPE, stdout=subprocess.PIPE, shell=True,
                close_fds=True)
        _set_nonblocking(p.stdout)
        self.rsh = p
        self.decoder = netstring.Decoder()

    def _not_implemented(self):
        raise NotImplementedError()

    store = _not_implemented
    load = _not_implemented
    delete = _not_implemented
    list_tags = _not_implemented
    get_last_tag = _not_implemented
    suggest_tag = _not_implemented
    fsck = _not_implemented

    def close(self):
        self.rsh.stdin.close()
        result = self.rsh.wait()
        if result != 0:
            raise RuntimeError('Remote process returned %d' % result)

    def _read_one_netstring(self):
        data = None
        while True:
            for s in self.decoder.feed(data):
                return s
            data = _read_some(self.rsh.stdout)
            if data == '':
                raise RuntimeError('Remote process closed unexpectedly')

    def _request(self, request):
        self.rsh.stdin.write(netstring.encode(request.SerializeToString()))
        self.rsh.stdin.flush()
        reply = tbl.ddar_pb2.Reply()
        reply.ParseFromString(self._read_one_netstring())
        return reply

    def _have_chunk(self, cursor, h):
        request = tbl.ddar_pb2.Request()
        request.have_chunk_request.sha256 = h
        reply = self._request(request)
        assert(reply.HasField('have_chunk_reply'))
        assert(reply.have_chunk_reply.sha256 == h)
        return _ImmediateRequest(reply.have_chunk_reply.have)

    def _store_chunk(self, member_id, cursor, data, offset, length,
                     sha256=None):
        if sha256 is None:
            sha256 = hashlib.sha256(data).digest()
        request = tbl.ddar_pb2.Request()
        r = request.store_chunk_request
        if data is not None:
            r.data = data
        r.sha256 = sha256
        r.offset = offset
        r.length = length
        reply = self._request(request)
        assert(reply.HasField('store_chunk_reply'))
        assert(reply.store_chunk_reply.sha256 == sha256)

    def _store_complete_member(self, cursor, h, length, member_id):
        request = tbl.ddar_pb2.Request()
        request.commit_request.sha256 = h
        request.commit_request.length = length
        reply = self._request(request)
        assert(reply.HasField('commit_reply'))
        return _ImmediateRequest(None)

    def store(self, tag, f=sys.stdin, aio=False, server=False):
        assert(not server)
        self._store(None, None, f, aio)

class OptionError(RuntimeError):
    def __init__(self, m):
        self.message = m

class OptionHelpRequest(Exception): pass

def parse_args(args):
    # The argument parser options are modelled after GNU ar, tar, gzip and
    # tarsnap. The aim is to not confuse or annoy anyone already familiar with
    # those tools by behaving as one would expect. Unfortunately I can't seem
    # to find a way to make this happen using argparse, optparse or getopt, so
    # doing it by hand it is.

    result = {}
    pos_args = []
    pos_arg_names = [ 'member' ]

    bool_options = set('ctxd') | set([ 'fsck', 'force-stdout', 'server' ])
    arg_options = set([ 'f', 'N', 'rsh' ])
    command_options = set([ 'c', 't', 'x', 'd', 'fsck' ])

    all_options = bool_options | arg_options | command_options
    single_letter_options = set([x for x in all_options if len(x) == 1])
    long_options = set([x for x in all_options if len(x) > 1])

    def set_result(k, v):
        if k in command_options and any((k in result for k in command_options)):
            raise OptionError('only one command option may be specified')
        result[k] = v

    def parse_long_option(arg_iter, arg):
        arg = arg.lstrip('-')

        if arg == 'help':
            raise OptionHelpRequest()

        if arg in arg_options:
            set_result(arg, arg_iter.next())
        elif arg in bool_options:
            set_result(arg, True)
        else:
            raise OptionError('unknown option: %s' % arg)

    def parse_option_cluster(arg_iter, arg):
        arg = arg.lstrip('-')
        while arg:
            if arg[0] in single_letter_options:
                if arg[0] in arg_options:
                    if len(arg) == 1:
                        set_result(arg[0], arg_iter.next())
                    else:
                        set_result(arg[0], arg[1:])
                    return
                elif arg[0] in bool_options:
                    set_result(arg[0], True)
                    arg = arg[1:]
                    continue
                else:
                    raise NotImplementedError()
            elif arg[0] == 'h':
                raise OptionHelpRequest()
            else:
                raise OptionError('unknown option: %s' % arg[0])

    arg_iter = iter(args)
    first = True
    while True:
        try:
            arg = arg_iter.next()
        except StopIteration:
            break
        if first and not arg.startswith('--'):
            # First arg is always the command even without a '-', like ps
            # and tar do it, unless it is a long option
            parse_option_cluster(arg_iter, arg)
        elif arg == '--':
            # Remaining args are all positional
            pos_args.extend(arg_iter)
            break
        elif not arg:
            # An empty arg? I guess it's positional
            pos_args.append(arg)
        elif arg == '-':
            # This is a positional arg too
            pos_args.append(arg)
        elif arg.startswith('--'):
            parse_long_option(arg_iter, arg)
        elif arg.startswith('-'):
            parse_option_cluster(arg_iter, arg)
        else:
            pos_args.append(arg)
        first = False

    result.update(zip(pos_arg_names, pos_args))

    last_pos_arg_name = pos_arg_names[-1]
    try:
        result[last_pos_arg_name] = [ result[last_pos_arg_name] ]
    except KeyError:
        result[last_pos_arg_name] = []
    result[last_pos_arg_name].extend(pos_args[len(pos_arg_names):])

    for opt in list(all_options) + pos_arg_names:
        if opt not in result:
            result[opt] = None

    return result

def main_add_one(store, filename, tag, server=False):
    if server:
        store.store(tag, None, server=True)
    elif filename == '-':
        store.store(tag, sys.stdin)
    else:
        with open(filename, 'rb') as f:
            store.store(tag, f, aio=True)

def main_add(store, members, tag=None, server=False):
    if not members:
        if not tag:
            try: tag = store.suggest_tag()
            except NotImplementedError: pass
        main_add_one(store, '-', tag, server=server)
    elif len(members) == 1:
        if not tag:
            tag = members[0]
        main_add_one(store, members[0], tag, server=server)
    else:
        for member in members:
            main_add_one(store, member, member, server=server)

def main_extract(store, members):
    if not members:
        members = [ store.get_last_tag() ]
    for tag in members:
        store.load(tag, sys.stdout)

def main():
    try:
        args = parse_args(sys.argv[1:])
        if not any((args[k] for k in (list('cxtd') + ['fsck']))):
            raise OptionError('a command is required')
        if not args['f']:
            try:
                args['f'] = args['member'].pop(0)
            except IndexError:
                raise OptionError('an archive must be specified')
        if args['d'] and not args['member']:
            raise OptionError('no member specified')
        if args['N'] and not args['c']:
            raise OptionError('option -N not valid except in create mode')

        if (':' in args['f'] or args['server']) and len(args['member']) > 1:
            raise OptionError('can only add one item at once to remote archive')

        if ':' in args['f']:
            if not args['c']:
                raise OptionError('remote archive only permitted with -c')
            server, filename = args['f'].split(':')
            remote_args = [ '-c', '-f', filename ]
            if args['N']:
                remote_args.extend(['-N', args['N']])
            if args['rsh']:
                rsh = args['rsh']
            else:
                rsh = 'ssh'
            archive = RemoteArchive(rsh=rsh, server=server, args=remote_args)
            args['f'] = filename
        else:
            archive = Archive(args['f'], auto_create=args['c'])

        if args['c']:
            main_add(archive, args['member'], args['N'], server=args['server'])
        elif args['x']:
            if not args['force-stdout'] and os.isatty(sys.stdout.fileno()):
                raise OptionError('output is a terminal and --force-stdout not specified')
            main_extract(archive, args['member'])
        elif args['d']:
            for member in args['member']:
                archive.delete(member)
        elif args['t']:
            for tag in archive.list_tags():
                print tag
        elif args['fsck']:
            if not archive.fsck():
                archive.close()
                print 'fsck returned errors'
                sys.exit(1)

        archive.close()

    except OptionHelpRequest:
        print '''ddar: store multiple files efficiently in a de-duplicated archive

Create or add to an archive:
    ddar [-]c [-f] archive [-N member-name] < file
    ddar [-]c [-f] archive [-N member-name] member
    ddar [-]c [-f] archive member [member...]

    If member-name is unspecified, ddar will use the supplied filename
    as the member name, or for stdin create a suitable name based on the
    current date.

Extract from an archive:
    ddar [-]x [options] [-f] archive > file  # extract the most recent member
    ddar [-]x [options] [-f] archive member-name > file

    Options:
        --force-stdout  Write to stdout even if stdout is a terminal

List members in an archive:
    ddar [-]t [-f] archive

Delete members from an archive:
    ddar [-]d [-f] archive member-name [member-name...]

Check an archive for integrity:
    ddar --fsck [-f] archive


Examples:
    Back up your home directory daily:
        tar c ~|gzip --rsyncable|ddar cf /mnt/external_disk/home_backup

    Restore your home directory after a disaster:
        ddar xf /mnt/external_disk/home_backup|tar xzC/
'''
        return
    except OptionError, e:
        print 'ddar: %s' % e.message
        print 'Try: ddar --help'
        return

if __name__ == '__main__':
    main()

# vim: set ts=8 sts=4 sw=4 ai et :
